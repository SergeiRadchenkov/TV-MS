'''
Произвести вычисления как в пункте 2, но с вычислением intercept. 
Учесть, что изменение коэффициентов должно производиться на каждом шаге одновременно 
(то есть изменение одного коэффициента не должно влиять на изменение другого 
во время одной итерации).
'''
import numpy as np

# Данные
zp = np.array([35, 45, 190, 200, 40, 70, 54, 150, 120, 110])
ks = np.array([401, 574, 874, 919, 459, 739, 653, 902, 746, 832])

# Параметры градиентного спуска
learning_rate = 0.000001
n_iterations = 10000

# Инициализация коэффициентов
b0 = 0
b1 = 0

# Градиентный спуск
for _ in range(n_iterations):
    y_pred = b0 + b1 * zp
    b0_gradient = -2 * np.sum(ks - y_pred) / len(zp)
    b1_gradient = -2 * np.sum(zp * (ks - y_pred)) / len(zp)
    
    b0 -= learning_rate * b0_gradient
    b1 -= learning_rate * b1_gradient

print(f'Коэффициенты линейной регрессии (градиентный спуск, с intercept): b0 = {b0}, b1 = {b1}')

# Коэффициенты линейной регрессии (градиентный спуск, с intercept): 
# b0 = 2.2907471768833725, b1 = 5.873019702381174